\documentclass[english]{report} \usepackage[english]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{graphicx}
\usepackage{here}
\usepackage{pdfpages}
\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{listings}
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{lmodern}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}

% \usepackage{caption} \DeclareCaptionFont{white}{\color{white}}
% \DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
% \captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}



% \RequirePackage{draftwatermark} \SetWatermarkText{DRAFT}

\begin{document}

\pagestyle{fancy}
\lhead{}%\color{red}\textbf{DRAFT}}
\chead{- Knowledge Based Systems - }
\rhead{\includegraphics*[width=2.0cm]{abbildungen/dhbwlogo.png}}

\cfoot{Christoph Schabert, Stephan Alaniz Kupsch, Jan Brodhacker (TINF11-AI\/BC) \\DHBW Mannheim}
\lfoot{}%\color{red}\textbf{DRAFT}}
\rfoot{\thepage}
\fancyhfoffset{\marginparsep}
\renewcommand{\footrulewidth}{1.0pt}
\renewcommand{\headrulewidth}{1.0pt}
\renewcommand{\headheight}{30pt}
\pagenumbering{arabic}





\begin{titlepage}

\begin{flushright}
    \includegraphics*[width=4.0cm]{abbildungen/dhbwlogo} \\ 
\end{flushright}
\begin{center}
\vspace{1.5cm}
\Huge{ \textsf{Knowledge Based Systems}} \\
	Yavalath\\
    \vspace{4cm}
 \normalsize{
    \begin{tabular}{ll}
    	Names: & {Schabert Christoph,  Alaniz Kupsch Stephan,  Brodhacker Jan} \\
    	Course: & {TINF-AI11/BC}	\\
    	Lecturer: &  {Hr. Dr. Vossen}\\
    	working period: & {09.09.2013 - 14.1.2014}
    \end{tabular}\\
    }
\end{center}

\end{titlepage}

\newpage 
\section*{Abstract}\thispagestyle{empty}
This paper was created during the lecture 'Knowledge Based Systems' at the
Baden-Wuerttemberg Cooperative State University in the winter semester 2013. It
describes the basic facts and rules about the board game Yavalath, which was
created by an artificial intelligence (AI) called LUDI. Yavalath was implemented
during this project with the addition of AIs, namely a UCT AI and a kind of a
random AI. Both the theoretical background as well as the implementational
approach are described in this paper.

\newpage
\section*{Statutory declaration}\thispagestyle{empty}
I herewith declare that I have completed the present thesis independently making
use only of the specified literature and aids. Sentences or parts of sentences
quoted literally are marked as quotations; identification of other references
with regard to the statement and scope of the work is quoted. The thesis in this
form or in any other form has not been submitted to an examination body and has
not been published.\\
\\
Stephan Alaniz Kupsch:\\
Mannheim, the 13.11.2013 \hspace{2.5cm} .............................. \\
\hspace*{6.5cm} (signature)\\
\\
Christoph Schabert:\\
Mannheim, the 13.11.2013 \hspace{2.5cm} .............................. \\
\hspace*{6.5cm} (signature)\\
\\
Jan Brodhacker:\\
Mannheim, the 13.11.2013 \hspace{2.5cm} .............................. \\
\hspace*{6.5cm} (signature)\\

\newpage
\setcounter{page}{1}



\tableofcontents

\listoffigures

%\renewcommand{\lstlistlistingname}{Listing of Sourcode}
%\renewcommand{\lstlistingname}{Sourcode}
%\lstlistoflistings

\bibliographystyle{plain}
\bibliography{Quellen}

\newpage

\chapter{Introduction}
\label{sec:chapter1}
\section{Goal and Motivation}
The goal of this project is to implement a game called Yavalath and to develop
two different artificial intelligences, which are able to play the game and win
against even good human players.

\section{Technical Notes}
All sources of illustrations are listed in the illustration register. All
illustrations that do not have an explicitly mentioned source are self-created by
the authors.


The implementation of the game and the AI-players with a fully functional
graphical user interface is availible on GitHub under the following url:
\begin{quotation}
    https://github.com/belafarinrod91/KBSYavalath (hep: 25.10.2013)
\end{quotation}
This project is under no license or otherwise protected, feel free to clone the
Git repository and compile it yourself.


This publication was written in \LaTeX \ and compiled with MikeTex.

\chapter{The Game - Yavalath}
\label{sec:chapter2}
This chapter will explain the creation and history of Yavalath.

\section{History}
Yavalath is the most successful game evolved from LUDI in 2007 and published in
2009 by Nestorgames (Spain). LUDI is a game-generating computer program that
implements an algorithm for generating new games. This LUDI software system not
only defines the scope and rules of the game, it also interprets the game and
coordinates testplays to measure the game's quality.
LUDI was developed by Cameron Browne as part of his PhD in AI and game design.
He was also awarded with the Dean's Award for Outstanding Thesis
\cite{EvolutionGameDesign}. \\

In 2011 Yavalath was ranked place 99 by the BoardGameGeek
community\footnote{\url{www.boardgamegeek.com}} of 4300 abstract games ever invented.
It was also ranked place 8 of 200 abstract games invented in 2007. \\

Yavalath won 2012 the Humies Award for Human-Competitive Result Produced by
Genetic and Evolutionary Computation.

\section{Categorisation}
Yavalath is a blend of connect- and patter-building games. The BoardGameGeek
community categorised Yavalath as an abstract game because it was the first
computer-generated game with such a huge success.

Yavalath also inspired a new game category that is called:
\begin{quote}
	winning with a line of N and losing with a line of N-1.
\end{quote}

The name Yavalath was randomly created by a Markovian algorithm.

\section{The Rules}
The rules for Yavalath are pretty simple what makes it easy to learn. But it is
still novel enough to be interesting. 
One of the reasons why the game became so successful is that Yavalath achieves
a good balance between simplicity and innovation.

Yavalath is for two or three players and has an average playing time of 10
minutes per game\footnote{this was voted by the BoardGameGeek comunity,
source:\url{http://boardgamegeek.com/boardgame/33767/yavalath}}.

The board is a hexagon with 5 spaces on each side, that is initially empty.
With these specifications there are 61 free cells in total as shown in the illustration \ref{fig:yav_board}.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Abbildungen/yav_emptyBoard.png}
\caption[The Yavalath game board, Source:\cite{yvalath}]{The Yavalath game board}
\label{fig:yav_board}
\end{figure}


Every turn a player places a piece of their color onto the board. 

This continues until:
\begin{itemize}
	\item one player has won
	\item all but one player have lost
	\item all cells are filled and it is a draw
\end{itemize}

As soon as a player forms a line of three pieces in a row the player loses.

A player can win by two different methods:
\begin{enumerate}
	\item one player manages to make a line of four or more pieces of his color (In illustration  \ref{fig:yav_4inarow} the white player won)
	\item one player forces another player to form a line of three pieces, which is called a forced move (In illustration  \ref{fig:yav_forceMove} the black player is forced to move to prevent the white player from winning)
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Abbildungen/yav_4inarow.png}
\caption[Four in a row winning, Source:\cite{yvalath}]{Four in a row wining}
\label{fig:yav_4inarow}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Abbildungen/yav_forceMove.png}
\caption[Forced move winning, Source:\cite{yvalath}]{Forced move winning}
\label{fig:yav_forceMove}
\end{figure}


This leads to a so called Rule Tension, in this case four in a row is good but
three in a row is bad. This means the player cannot just extend lines and must
think about the pros and cons of every move.\cite{yvalath}

\section{Tactics and Strategy}
Cameron Browne himself defined some tactics and strategies and described them in
his publication. He started to describe a number of choices for the first move
and a number of patterns that have a good chance of winning.


The first move is crucial for the following tactics and for the chance to win.
Illustration \ref{fig:yav_winChance} shows a Yavalath board with the win chances
for the first move, a bigger bubble means a bigger win chance. It visualizes
that a centered first move provides the biggest chances of
winning.\cite{yvalath}


\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Abbildungen/yav_winChance.png}
\caption[The chances of winning for the first Move, Source:\cite{yvalath}]{The win chance for the first Move}
\label{fig:yav_winChance}
\end{figure}


Another good choice for a first move is to start with a pattern. The triangle
starting pattern is an excellent example for a first move pattern (the triangle
starting pattern is shown in illustration \ref{fig:yav_triangle} on the left
side) because it provides a huge number of chances to win by a forced move.
Additionally, it is hard to stop a player that used the triangle pattern as a
starting move. The only way to stop it is to foresee it pretty early and prevent
the player from forming the triangle.
The right side of illustration \ref{fig:yav_triangle} shows a possible outcome
of the triangle starting position where the black player has lost by a forced
move.\cite{yvalath}

\begin{figure}[H]
\centering
\includegraphics[width=0.45\textwidth]{Abbildungen/yav_triangle.png}
\caption[The triangle starting pattern, Source:\cite{yvalath}]{The triangle starting pattern}
\label{fig:yav_triangle}
\end{figure}

In Yavalath a triangle is always a good pattern like the small triangle (as seen
in illustration \ref{fig:yav_smallTriangle}). The Reason that the small triangle
is such a strong pattern is because it can form a huge number of winning
chances.


\begin{figure}[H]
\centering
\includegraphics[width=0.35\textwidth]{Abbildungen/yav_smallTriangle.png}
\caption[A small triangle pattern, Source:\cite{yvalathBGG}]{A small triangle pattern}
\label{fig:yav_smallTriangle}
\end{figure}

To show that the small triangle is a good pattern, a winning pattern is shown in
illustration \ref{fig:yav_smallTriangleWin1}. This forces the black player to
lose by a forced move. It is also possible to turn a pattern by 120 degrees. With
that in mind there are at least three possible winning situations with this
simple pattern. If an opponent player tries to form a triangle it is recommended
to stop him as soon as possible.\cite{yvalathBGG}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Abbildungen/yav_smallTriangleWin1.png}
\caption[A small triangle winning pattern, Source:\cite{yvalathBGG}]{A small triangle winning pattern}
\label{fig:yav_smallTriangleWin1}
\end{figure}

\chapter{Artificial Intelligences}
\label{sec:artificIntelligences}
The following chapter describes the two approaches of artificial intelligences,
which were implemented for this game. The two artificial intelligences are able
to play and win against a human player. On the one hand there is the so called
UTC AI, which has three different levels of difficulty. On the other hand
there is the simple approach of a random AI. Both approaches are explained in a
more detailed view in the next two sections.



\section{Random-AI}
The random AI is one of the two implemented artificial intelligences, as already
mentioned in the description of this chapter.
The random AI is definitely the easier opponent for a human player
than the other kind of AI, the UCT AI.
Additionally to this fact, the random AI also has an easier approach to
implement. Rather the random AI can be described within two sentences:
The random AI sets its token randomly, but before it does this, it checks if it
could win with setting a token to a specific field, if it must prevent an
opponent from winning or if it would lose with setting it on a specific field.
After this regard, the random AI sets its token completely
randomly to the playground. This happens with the assistance of a special
algorithm, which receives the system time. Based on this system time the
algorithm can determine a special courton on the playground. If this courton is
not used by another player, the random AI will set its token there. It doesn't
matter if this turn was smart or if it could yield an advantage.


But before the random AI 
sets its token, it checks whether a opponent could win on the next turn. This
happens based on a predefined rule set, as already mentioned in the chapters
above. So figure \ref{fig:randomAI1} shows a situation, where the red player
(random AI) prevents the win of player green, by setting the red token with the
white dot.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{Abbildungen/randomAI1.png}
\caption{Random AI prevents red player from winning }
\label{fig:randomAI1}
\end{figure}
Additionally to that, the random AI checks also if it would lose with setting
the token on the specified position. Figure \ref{fig:randomAI2} shows such a
situation. If the algorithm would return a field next to one of the two red
tokens, the random AI would not set this token, because then it would lose the
game. So the random AI would set its token somewhere else.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{Abbildungen/randomAI2.png}
\caption{Random AI sets token other position}
\label{fig:randomAI2}
\end{figure}
And all good things are three, so the random AI checks in a third step if it
could win itself, when it sets the token on a certain courton. Figure
\ref{fig:randomAI3} shows that the random AI would recognize the chance to win
in this situation. The random AI must only set the red token with the white dot
to win.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{Abbildungen/randomAI3.png}
\caption{Random AI recognizes chance to win }
\label{fig:randomAI3}
\end{figure}
All in all the random AI is not very smart, because neither can it deal with
specific game events nor can it learn from faults, which were made by the random AI
in a game before. It actually sets its token randomly as long as the AI itself
has not lost yet.
The probability of winning is not very high. All in all the random AI is not only a
random AI as described before, but it has implemented some logic to
recognize if the random AI could win or prevent a win of an other player.


\section{UCT-AI}
\label{sec:chapter4}
The following chapter describes the fundamentals of the UCT (Upper Confidence
Bounds applied to Trees) algorithm. It was implemented in order to provide the
Yavalath game with a strong artificial intelligence (AI) to play against. The main
task of a game playing AI is to choose its next move with the information about
the state of the game. The motivation and reasons behind choosing the UCT
algorithm are briefly pointed out. Afterwards it is explained how this
algorithms works and how it works in regard to Yavalath.

\subsection{Why UCT?}
The UCT algorithm was developed for decision making problems especially in terms
of game playing. It has its strengths in games where:
\begin{itemize}
	\item the state of a game is hard to evaluatate and
	\item the game has a high branching factor.\cite{wiki:mcts}
\end{itemize}

In Yavalath, these two attributes can be found. Firstly, Yavalath has no
developed theory, mainly because it is relatively unknown. Therefore, there
does not exist an evaluation function which could rate the current state of the
game. If it was possible to assign a score depending on how good a player's
tokens are placed on the board, it would be possible to deploy algorithms (such
as Alpha-beta pruning and Minimax), which can leverage this information in order
to find a good or even the best possible next move.\\

Secondly, Yavalath also has a high branching factor. Looking only at the first
10 moves (5 from each player when 2 are playing), there already are
\(\frac{61!}{51!}=3,27*10^{17}\) different move combinations. Good game playing
AIs cannot only look at the next move, but rather need to look ahead a couple of
moves. Otherwise, they could not detect potential threats or opportunities which
might lie ahead a few moves. In order to investigate all these branches, the AI
needs to be capable of handling the huge number of combinational moves.\\

The UCT algorithm is able to handle both these attributes of Yavalath and
therefore is a reasonable choice for a AI algorithm. This is further justified
by the fact that the creator of Yavalath himself implemented this algorithm as
AI in his Yavalath application.\cite{yvalath}


\subsection{How the UCT algorithm works}

The UCT algorithm is based on the Monte Carlo Tree Search (MCTS). The MCTS is a
best-first search algorithm where the results from previous iterations guide the
algorithm to the best path through the tree. It consists of 4 steps which are
run in a loop. The 4 steps\cite{mcts}\cite{wiki:mcts} are:
\begin{itemize}
	\item \textit{Selection:} Starting at root node R, successively select optimal child nodes until a leaf node L is reached.
	\item \textit{Expansion:}  If L is a not a terminal node, which ends the game, then create one or more child nodes and select one C.
	\item \textit{Simulation:}  Run a simulated playout (usually a random playout) from C until a result is achieved.
	\item \textit{Backpropagation:}  Update the current move sequence from C to R with the simulation result.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{Abbildungen/MonteCarloTreeSearch.png}
\caption[Monte Carlo Tree Search, Source:\cite{paper:mcts}]{Monte Carlo Tree Search}
\label{fig:mcts}
\end{figure}

In the regard of Yavalath, the root of the tree would represent the current
state of the game. From there the first layer holds all possible next moves the
AI can make. The goal of the AI is now to find the best promising move in that
first layer. Going down one layer, the second layer would represent all possible
counter moves of the following player. In each layer the algorithm tries to find
the best move and expands successive moves to see how the game would evolve when
picking a particular move in the first place. That said, the algorithm also
takes into consideration what the best move for the AI's opponent is. For example,
even though a move seems to be promising in the first layer, the opponent might
be able to counter this move with a very strong response move and thus leading
to a worse situation than initially thought. Therefore, it is important to
further investigate potentially good moves by going down more and more layers,
i.e. rounds of the game. The simulation phase is used to determine the
performance of the moves. Usually, random playouts are used to quickly see how a
game of chosen moves turns out in the end. It might seem that a random playout
cannot be representative, but with more and more playouts the accuracy of the
expected win chance increases due to the law of large numbers.
The result of a random playout is backpropagated to all involved moves till the
root node.\\

It is still unclear how the algorithm chooses a node, and path
respectively, in the selection phase. This is probably the most important part
of the algorithm because it determines the best-first search strategy. The
algorithm needs to find a good balance between exploitation and exploration.
Expoitation is defined as investigating deep variants of the best move sequences
whereas exploration means trying out moves which have not been simulated many
times. The former is important because the AI needs to be confident that the
choice of a move is good even in the further playout and the latter has to be
carried out so that the AI does not miss out on good moves which might just have
unluckily performed bad in a few playouts.\\

To balance out exploitation and exploration the UCB1 (Upper Confidence Bound 1)
policy is used which was first introduced by Peter Auer.\cite{paper:ucb1}
Before using UCB1 in order to decide which node to select from a set of nodes,
every node has to be played out at least once. Afterwards the \(i\)th node is
chosen which maximizes the equation \ref{eq:UCB1}.

\begin{equation}
\label{eq:UCB1}
\bar{x}_{i} + C * \sqrt{\frac{\ln n}{n_{i}}}
\end{equation}

\(\bar{x}_{i}\) is the performance measure from node \(i\). The performance measure
must lie in the interval [0;1]. For games it usually is the win chance
\(\frac{w_{i}}{n_{i}}\) where \(w_{i}\) is the number of wins from random
playouts of node \(i\) and \(n_{i}\) is the number of times node \(i\) has
already been played. \(n\) is the overall number of playouts and \(C\) is a
constant which has to be adjusted empirically for a given problem.\cite{thesis:uct}


In the UCB1 policy, the first part of the equation \(\bar{x}_{i}\) stands for the
exploitation part. The higher the win chance of a move, the higher the value of
the equation. The rest of the equation stands for the exploration part. Assuming
a node is visited once and not any more for a couple of times, i.e. \(n_{i}\) is
fixed, then only \(n\) increases which makes the overall second part of the
equation grow. On the contrary, for a node which is often visited \(n_{i}\) is
increased simultaneously. As a result, the second part of the equation decreases
because the linear function in the denominator rises faster than the natural
logarithm in the numerator. This means that nodes with a bad win chance and few
plays will eventually be explored too. To what degree and how often they are
explored determines the constant \(C\). The higher the value the more
exploration is done and the lower the value the more exploitation is done.
Auer suggested to set the constant \(C\) to
\(\sqrt{2}\).\cite{paper:ucb1} Therefore, this can be used as a
reasonable starting value.


From the UCB1 policy it can be derived that each node needs to to hold two
pieces of information: the number of wins and the number of visits.
Both the MCTS and the UCB1 policy form the UCT algorithm. This explains where
the name Upper Confidence Bounds applied to Trees comes from. The UCT algorithm
could be run quite long on highly branching trees. Fortunately, it can be
stopped at any time. Of course, the longer the algorithm runs the more
statistically significant the result will be, but often a fixed number of
iterations (depending on the problem) are sufficient to get a good result.
The best move will finally be chosen by looking at the information of all nodes
in the first layer. It is best to choose the node which was visited the most.
Even though it is possible to choose the node with the highest win chance, this
might result in a bad move. Through exploration a node can temporarily have a
higher win chance, but as it is probably not exploited very much, it is unclear
if it really is a good move. A high win chance will subsequently result in
higher exploitation and a higher number of visits. Therefore, the number of
visits should be used to make the decision on the best move.


\chapter{Technical Implementation}
\label{sec:technImplementation}
The game was implemented in Java, a platform-independent programming language.
It offers many extensions to develop such a complex game. The aspect that Java
is a object oriented programming language provides many advantages to model
useful classes, which can demonstrate the structure of such a game. Java is also
fast enough to calculate all dependencies and return a
response to the user's interactions with the game in a suitable time. In this
context no frameworks were used. The graphical user interface was realized with
the assistance of the GUI-Library SWING for Java, which is based on the
Abstract-Window-Toolkit (AWT).
\section{implemented Features}
The following section describes the features, which were implemented. \\
After the game is started, the main menu will be opened. The user can start a
new game, choose settings and exit the game in this menu. Within the settings menu
the user is able to define how many player are part of the game (from two to
three) and which sort of player each defined player is (human player, random AI
or UCT AI). When a player is a UCT AI an additional option is available, namely,
which level of difficulty the UCT AI has.
\newpage

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{Abbildungen/exampleOfPlayGround.png}
\caption[Playground of Yavalath implementation]{Playground of Yavalath implementation}
\label{fig:playgroundOfYav}
\end{figure} 

\noindent Figure \ref{fig:playgroundOfYav} shows an example of a typical
playground of the Yavalath implementation.
The upper left corner shows some general game information. Which player has
which color and what kind of type the different players are. Beneath this
information the field 'Move' shows, which player's move it is, with the specific
color. The field in the lower left corner shows the calculated win chance of the
UTC AI, presented in percent. If there are multiple UCT AI in the game, the
certain UCT AI can be indicated by the reference to the color.  \\
The main part of the window is used for the real playground. When it is a human
player's turn, the player is able to click on the court,
where he wants to set his 
token. 

\section{Implementation of the UCT-AI}
The implementation of the UCT-AI is primarily based on the theoretical
foundation of the Monte Carlo Tree Search and the UCB1 policy explained in the
previous chapter. This section deals with implementational details such as how
the algorithm was tweaked and how the implementation differs from the theory. 
Sensei's Library features pseudo code for the UCT algorithm which was used as a
basis.\cite{sl}


The selection phase of the MCTS uses the UCB1 policy as described before. The
main adjustment done in this part is the choice of the constant \(C\) in the
UCB1 policy from equation \ref{eq:UCB1}. In Sensei's Library, it is suggested to
use a value of \(\frac{1}{sqrt{5}}\).\cite{sl} As this is a smaller
number than \(sqrt{2}\) suggested by Auer, it results in a more exploitational
tree search. Therefore, good moves are investigated more than exploring the
total width of the tree. It turns out that this works very well with Yavalath.\\

The expansion phase is slightly modified. Firstly, when expanding the tree by a
new layer, all children nodes of the currently expanded node are added to the
tree. From the new nodes, a random node is selected. If the path to the newly
added layer is chosen again, each of the nodes is visited at least once before a
definite selection by the UCB1 policy is made. The Monte Carlo Tree Search
suggests to expand the tree as soon as a leaf node is reached. This was modified
by the fact that each node needs to be visited at least 10 times before it is
expanded with its children. This means that the simulation phase would start
right after the selection phase if a leaf node has less than 10 visits. The
reason behind this is to reduce to size of the tree stored in memory and
therefore make the algorithm less prone to run out of memory. The number 10 was
again suggested by the Sensei's Library and works very well in this regard, too.\\

The simulation phase is done with the help of the Random-AI described earlier.
Depending on the number of players, the same number of Random-AI's finish the
game. This phase is the main reason behind some AI logic implemented in the
Random-AI. If the Random-AI would choose its moves completely randomized, it
would also reduce the effectiveness and strength of the UCT-AI. The reason
behind this is that games at the later stage tend to have lots of fields on the
board where a player can lose by placing a third token in a row. The problem
here is that a human player would most likely never do such a move if it is not
forced. Therefore, many simulated games would result in not representative
outcomes, even though it is way more interesting how the game turns out if these
moves are avoided. The same holds for following suit of a forced move or taking
a winning move which is also done by the Random-AI. Having this added logical
improvements in the Random-AI, the simulations done in the UCT algorithm become
much more valuable and representative so that the overall AI increases in
strength.\\

For the backpropagation phase, there is nothing to adjust or modify. It is only
important to keep in mind that for each player the outcome of the simulation is
different (expect for a draw). When having two players, the result of the
simulation has to be alternated on each layer / node when doing the
backpropagation. Accordingly, the result has to be matched with the player of
the current node when dealing with three players.\\

Finally, there are two different ways to execute the UCT algorithm. First, one
can set a fixed time and let the algorithm do as many iterations of the 4 steps
as possible. Second, one can set a fixed number of iterations and have the
algorithm run a somewhat undefined time depending on the power of the CPU on
which it runs. It was decided to implement the second option, because this
enables to define different difficulties (i.e. strengths) of the UCT-AI.
The more iterations there are, the more exploitation as well as exploration the
UCT-AI could do and thus resulting in a better choice of move. Less iterations
mean a weaker AI whereas more iterations make a strong AI. Option one with fixed
ttime does not guarantee the same nummber of iterations on different CPUs and
herefore, the AI would not perform consistently on every PC. In addition, there
should not be a huge difference in execution time, provided that the PC's CPU is
not too outdated. \\

Having experimented and played against the UCT-AI the following number of
iteration were chosen for the corresponding difficulties:
\begin{itemize}
	\item \textit{Easy:} 10000 simulations
	\item \textit{Medium:}  20000 simulations
	\item \textit{Hard:}  50000 simulations
\end{itemize}


\chapter{Conclusion}
Yavalath is a enjoyable game considering that it was created by a machine. LUDI
itself is a very interesting approach in the field of artificial intelligence.
Yavalath has a very varying gameplay despite the fact that the rule set is very
simple. Additionally, move combinations and patterns add in strategic part to
Yavalath. Although the implemented Random AI will not challange a human player,
who knows these winning patterns, the UCT AI definitely does. The UCT AI turns
out to be a very good opponent even when good move combinations are known by the
human player. The AI knows how to counter these strategies and often penalizes
the smallest mistake with a instant lose for the player. Of course, the
different difficulties of the UCT AI make room for inexperienced player. Still,
the strongest UCT AI will challange even good Yavalath players.

% #TODO: Tech the Dokument



\end{document}
